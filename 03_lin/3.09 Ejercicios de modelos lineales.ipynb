{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.01 Clasificadores generativos vs discriminativos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** En relación con las ventajas de los clasificadores generativos y discriminativos, indica la afirmación incorrecta:\n",
    "1. Los generativos son, por lo general, más fáciles de ajustar.\n",
    "2. Los discriminativos suelen ofrecer mejor precisión que los generativos.\n",
    "3. Los generativos son más flexibles en preproceso de características.\n",
    "4. Los generativos facilitan el tratamiento de datos perdidos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 3; los discriminativos son más flexibles en preproceso de características."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.02 Clasificadores naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.03 Análisis discriminante Gaussiano"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Indica la afirmación incorrecta (o escoge la última opción si las tres primeras son correctas):\n",
    "1. Análisis discriminante Gaussiano asume que las densidades condicionales de las clases son Gaussianas independientes, lo que resulta en discriminantes (log-posteriors) cuadráticas.\n",
    "2. Análisis discriminante lineal asume que las densidades condicionales de las clases son Gaussianas de matriz de covarianzas común, lo que resulta en discriminantes (log-posteriors) lineales.\n",
    "3. LDA coincide con regresión logística (multinomial) en términos de modelo, pero se diferencia mucho en entrenamiento.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.04 Introducción a regresión logística"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Regresión logística es un modelo de clasificación $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})$, donde $\\boldsymbol{x}\\in\\mathbb{R}^D$ es un vector de $D$ características, $y\\in\\{1,\\dotsc,C\\}$ es la etiqueta de clase y $\\boldsymbol{\\theta}$ es un vector de parámetros. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas):\n",
    "1. Si $C=2$, el modelo es de regresión logística binaria.\n",
    "2. Si $C>2$, el modelo es de regresión logística multinomial.\n",
    "3. Si $C>2$, el modelo es de regresión logística multiclase.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.05 Regresión logística binaria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema:** Sea un problema de clasificación en dos clases, $y\\in\\{0,1\\}$, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$ un modelo de regresión logística binaria con $\\boldsymbol{w}=(3,3)^t$ y $b=0$. Determina la probabilidad de que $\\boldsymbol{x}=(0.5,0.5)^t$ pertenezca a la clase $1$ según el modelo dado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})%\n",
    "=\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b)=\\sigma((3, 3)(0.5, 0.5)^t+0)=\\dfrac{1}{1+e^{-3}}=0.9526$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema:** Sea un problema de clasificación en dos clases, $y\\in\\{0,1\\}$, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$ un modelo de regresión logística binaria con $\\boldsymbol{w}=(3,3)^t$ y $b=0$. Determina la logit, $f(\\boldsymbol{x}; \\boldsymbol{\\theta})$, así como la frontera y regiones de decisión que induce (con pérdida 0-1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad$ Logit: $\\quad f(\\boldsymbol{x}; \\boldsymbol{\\theta})=b+\\boldsymbol{w}^t\\boldsymbol{x}=3 x_1+3 x_2$\n",
    "\n",
    "Frontera: $\\quad 3 x_1+3 x_2=0\\to x_2=-x_1$\n",
    "\n",
    "Regiones de decisión:\n",
    "$$\\begin{align*}\n",
    "  \\mathcal{R}_1%\n",
    "  &=\\{\\boldsymbol{x}:p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})>p(y=0\\mid\\boldsymbol{x};\\boldsymbol{\\theta})\\}\\\\%\n",
    "  &=\\left\\{\\boldsymbol{x}:\\frac{p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})}{p(y=0\\mid\\boldsymbol{x};\\boldsymbol{\\theta})}>1\\right\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:f(\\boldsymbol{x};\\boldsymbol{\\theta})>0\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:3 x_1+3 x_2>0\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:x_2>-x_1\\}\\\\[3mm]%\n",
    "  \\mathcal{R}_0%\n",
    "  &=\\{\\boldsymbol{x}:x_2<-x_1\\}%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Sea $\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n): \\boldsymbol{x}_n\\in\\mathbb{R}^D, y_n\\in\\{0,1\\}\\}$, un conjunto de muestras no linealmente separables. Se quiere aprender un modelo de clasificación $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})$ que las clasifique sin error (pérdida 0-1 nula). Indica la respuesta correcta:\n",
    "1. Podemos usar regresión logística binaria convencional, esto es, con $p(y=1\\mid\\boldsymbol{x})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$.\n",
    "2. No podemos usar regresión logística binaria convencional, pero sí regresión logística binaria con una función de preproceso, $\\phi(\\boldsymbol{x})$, que linearice las muestras.\n",
    "3. No podemos usar regresión logística binaria, convencional o no, pues necesitamos un modelo de clasificación multi-clase.\n",
    "4. No podemos usar regresión logística binaria, convencional o no, pues es un modelo lineal y necesitamos uno no lineal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 2; la 4 es falsa porque puede haber una función de preproceso que linearice las muestras."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema:** Demuestra que el gradiente de\n",
    "$$\\operatorname{NLL}(\\boldsymbol{w})=-\\frac{1}{N}\\sum\\nolimits_n y_n\\log\\mu_n+(1-y_n)\\log(1-\\mu_n)$$\n",
    "con $\\mu_n=\\sigma(a_n)$, es \n",
    "$$\\nabla_{\\boldsymbol{w}}\\operatorname{NLL}(\\boldsymbol{w})=\\frac{1}{N}\\sum\\nolimits_n(\\mu_n-y_n)\\boldsymbol{x}_n$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\operatorname{NLL}}{\\partial w_d}%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "y_n\\frac{\\partial}{\\partial w_d}\\log\\mu_n%\n",
    "+(1-y_n)\\frac{\\partial}{\\partial w_d}\\log(1-\\mu_n)\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "y_n\\frac{1}{\\mu_n}\\frac{\\partial}{\\partial w_d}\\mu_n%\n",
    "-(1-y_n)\\frac{1}{1-\\mu_n}\\frac{\\partial}{\\partial w_d}\\mu_n\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "\\left[\\frac{y_n}{\\mu_n}-\\frac{1-y_n}{1-\\mu_n}\\right]%\n",
    "\\frac{\\partial}{\\partial w_d}\\mu_n\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "\\frac{y_n(1-\\mu_n)-(1-y_n)\\mu_n}{\\mu_n(1-\\mu_n)}%\n",
    "\\frac{\\partial}{\\partial a_n}\\sigma(a_n)\\frac{\\partial a_n}{\\partial w_d}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "\\frac{y_n-y_n\\mu_n-\\mu_n+y_n\\mu_n}{\\mu_n(1-\\mu_n)}%\n",
    "\\mu_n(1-\\mu_n)x_{nd}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n(y_n-\\mu_n)x_{nd}%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Sea el algoritmo Perceptrón (en clasificación binaria) aplicado a una muestra $(\\boldsymbol{x}_n,y_n)$ para actualizar el vector de parámetros actual, $\\boldsymbol{w}_i$ (supón $x_{n0}=1$ y $b=w_{i0}$) con factor de aprendizaje $\\eta_i=1$. Si $y_n=0$ y $\\hat{y}_n=\\mathbb{I}(\\boldsymbol{w}_i^t\\boldsymbol{x}>0)=1$, entonces:\n",
    "1. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i$\n",
    "2. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i+\\boldsymbol{x}_n$\n",
    "3. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i-\\boldsymbol{x}_n$\n",
    "4. Ninguna de las anteriores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.06 Regresión logística multinomial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
