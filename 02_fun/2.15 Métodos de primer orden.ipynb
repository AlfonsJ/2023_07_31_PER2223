{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de primer orden\n",
    "\n",
    "Los **métodos de primer orden** son métodos iterativos basados en derivadas de primer orden del objetivo. Dado un punto de inicio $\\boldsymbol{\\theta}_0$, la iteración $t$ consiste en hacer un paso de actualización (de $\\boldsymbol{\\theta}$):\n",
    "$$\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\eta_t\\boldsymbol{d}_t$$\n",
    "donde $\\eta_t$ es el **tamaño de paso (step size)** o **factor de aprendizaje (learning rate)**, y $\\boldsymbol{d}_t$ es una **dirección de descenso** como el negativo del **gradiente**, dado por $\\boldsymbol{g}_t=\\mathbf{\\nabla}\\mathcal{L}(\\boldsymbol{\\theta})\\rvert_{\\boldsymbol{\\theta}_t}$.\n",
    "Los pasos de actualización se suceden hasta que el método alcanza un punto estacionario, esto es,\n",
    "de gradiente nulo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirección de descenso\n",
    "\n",
    "Decimos que $\\boldsymbol{d}$ es una **dirección de descenso** si existe un $\\eta_{\\text{max}>0}$ tal que\n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta}+\\eta\\boldsymbol{d})<\\mathcal{L}(\\boldsymbol{\\theta})%\n",
    "\\qquad\\text{para todo}\\;\\eta\\in(0, \\eta_{\\text{max}})$$\n",
    "El gradiente en $\\boldsymbol{\\theta}_t$, $\\boldsymbol{g}_t=\\mathbf{\\nabla}\\mathcal{L}(\\boldsymbol{\\theta})\\rvert_{\\boldsymbol{\\theta}_t}=\\mathbf{\\nabla}\\mathcal{L}(\\boldsymbol{\\theta}_t)=\\boldsymbol{g}(\\boldsymbol{\\theta}_t)$ apunta en la dirección de máximo crecimiento de $\\mathcal{L}$, mientras que su negativo lo hace en la de máximo decrecimiento. En general, $\\boldsymbol{d}_t$ es dirección de descenso si $\\boldsymbol{d}_t^t\\boldsymbol{g}_t<0$, esto es, si $\\boldsymbol{d}_t$ y $\\boldsymbol{g}_t$ forman un ángulo mayor de $90$ grados (en el plano que los contiene). Este resultado incluye $\\boldsymbol{d}_t=-\\mathbf{B}_t\\boldsymbol{g}_t$ para toda matriz $\\mathbf{B}_t$ definida positiva.\n",
    "\n",
    "**Descenso más pronunciado (steepest descent)** escoge el negativo del gradiente como dirección de descenso:\n",
    "$$\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t-\\eta_t\\boldsymbol{g}_t$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\mathcal{L}=\\theta^2$, $\\theta_0=10$, $\\eta_t=0.2$, tolerancia $0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "3.6\n",
      "2.16\n",
      "1.296\n",
      "0.778\n",
      "0.467\n",
      "0.28\n",
      "0.168\n",
      "0.101\n",
      "0.06\n",
      "0.036\n",
      "0.022\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grad, theta, eta, tol = lambda t: 2*t, 10.0, 0.2, 0.01\n",
    "while True:\n",
    "\tdelta = -eta * grad(theta)\n",
    "\tif np.all(np.abs(delta) <= tol):\n",
    "\t\tbreak\n",
    "\ttheta += delta\n",
    "\tprint(round(theta, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
