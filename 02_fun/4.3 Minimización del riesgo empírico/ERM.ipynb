{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimización del riesgo empírico\n",
    "\n",
    "## Definición\n",
    "\n",
    "Empirical risk minimization (ERM) generaliza MLE sustituyendo la log-pérdida, \n",
    "$\\ell(\\boldsymbol{y}_n, \\boldsymbol{\\theta}; \\boldsymbol{x}_n)%\n",
    "=-\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{x}_n, \\boldsymbol{\\theta})$, \n",
    "por una pérdida genérica:\n",
    "$$\\hat{\\boldsymbol{\\theta}}_{\\text{erm}}=\\operatorname*{argmin}_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta})%\n",
    "\\quad\\text{con}\\quad%\n",
    "\\mathcal{L}(\\boldsymbol{\\theta})%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\ell(\\boldsymbol{y}_n, \\boldsymbol{\\theta}; \\boldsymbol{x}_n)$$\n",
    "En clasificación es usual considerar la **pérdida 01** de un clasificador $f(\\boldsymbol{x}_n; \\boldsymbol{\\theta}):$\n",
    "$$\\ell_{01}(\\boldsymbol{y}_n, \\boldsymbol{\\theta}; \\boldsymbol{x}_n)%\n",
    "=\\begin{cases}\n",
    "  0 & \\text{si $\\boldsymbol{y}_n=f(\\boldsymbol{x}_n; \\boldsymbol{\\theta}$})\\\\%\n",
    "  1 & \\text{si $\\boldsymbol{y}_n\\neq f(\\boldsymbol{x}_n;\\boldsymbol{\\theta})$}%\n",
    "\\end{cases}$$\n",
    "Así, el riesgo empírico se reduce al **error de clasificación** (en entrenamiento):\n",
    "$$\\mathcal{L}_{01}(\\boldsymbol{\\theta})%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\ell_{01}(\\boldsymbol{y}_n, \\boldsymbol{\\theta}; \\boldsymbol{x}_n)$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pérdida subrogada\n",
    "\n",
    "Llamamos pérdida subrogada a cualquier cota superior de la pérdida 01 que sea ajustada y más fácil de optimizar. Se suelen usar en clasificación binaria, con etiquetas $\\tilde{y}\\in\\{-1, 1\\}$ y clasificador de log-odds $\\eta=f(\\boldsymbol{x}; \\boldsymbol{\\theta})$\n",
    "$$p(\\tilde{y}\\mid\\boldsymbol{x},\\boldsymbol{\\theta})=\\sigma(\\tilde{y}\\eta)=\\frac{1}{1+e^{-\\tilde{y}\\eta}}$$\n",
    "$\\boldsymbol{x}$ se clasificará correctamente si $\\tilde{y}=-1$ y $\\eta<0$, o $\\tilde{y}=1$ y $\\eta>0$; esto es, si $\\tilde{y}\\eta>0$. Así pues, $\\tilde{y}\\eta$ puede verse como un **margen de seguridad:** debe ser positivo para que no haya error de clasificación y, cuanto mayor sea, mayor seguridad nos ofrece el clasificador. Por el contrario, si el margen es negativo,\n",
    "se producirá error de clasificación y, cuanto menor sea, menor seguridad tendremos en el clasificador.\n",
    "\n",
    "Consideremos la pérdida 01 en función del margen $\\tilde{y}\\eta$:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
