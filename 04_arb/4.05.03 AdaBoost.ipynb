{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pérdida exponencial y AdaBoost\n",
    "\n",
    "Si $\\ell(\\tilde{y},\\hat{y})=\\exp(-\\tilde{y}\\hat{y})$, $\\tilde{y}\\in\\{-1,+1\\}$, el objetivo FSAM en la iteración $m$ es:\n",
    "$$\\begin{align*}\n",
    "L_m(\\beta,\\boldsymbol{\\theta})%\n",
    "&=\\sum_{i=1}^N \\exp(-\\tilde{y}_i(f_{m-1}(\\boldsymbol{x}_i)+\\beta F(\\boldsymbol{x}_i;\\boldsymbol{\\theta})))\\\\%\n",
    "&=\\sum_{i=1}^N w_{im}\\exp(-\\beta\\tilde{y}_i F(\\boldsymbol{x}_i;\\boldsymbol{\\theta}))%\n",
    "\\qquad\\text{con}\\qquad w_{im}=\\exp(-\\tilde{y}_i f_{m-1}(\\boldsymbol{x}_i))\\\\%\n",
    "&=e^{-\\beta}\\sum_{\\tilde{y}_i=F(\\boldsymbol{x}_i;\\boldsymbol{\\theta})} w_{im}%\n",
    "+e^{\\beta}\\sum_{\\tilde{y}_i\\neq F(\\boldsymbol{x}_i;\\boldsymbol{\\theta})} w_{im}\\\\%\n",
    "&=(e^{\\beta}-e^{-\\beta})\\sum_{i=1}^N w_{im}\\mathbb{I}(\\tilde{y}_i\\neq F(\\boldsymbol{x}_i;\\boldsymbol{\\theta}))%\n",
    "+e^{-\\beta}\\sum_{i=1}^N w_{im}\n",
    "\\end{align*}$$\n",
    "La minimización de $L_m$ puede hacerse en dos pasos. Primero hallamos $\\boldsymbol{\\theta}_m$ a partir de los datos ponderados:\n",
    "$$\\boldsymbol{\\theta}_m=\\operatorname*{argmin}_{\\boldsymbol{\\theta}}%\n",
    "\\sum_{i=1}^N w_{im}\\mathbb{I}(\\tilde{y}_i\\neq F(\\boldsymbol{x}_i;\\boldsymbol{\\theta}))$$\n",
    "y luego $\\beta_m$ mediante minimización en $\\beta$ de $L_m(\\beta,\\boldsymbol{\\theta}_m)$:\n",
    "$$\\beta_m=\\operatorname*{argmin}_{\\beta}\\;L_m(\\beta,\\boldsymbol{\\theta}_m)%\n",
    "=\\frac{1}{2}\\log\\frac{1-\\operatorname{err}_m}{\\operatorname{err}_m}%\n",
    "\\qquad\\text{con}\\qquad\\operatorname{err}_m=\\frac{1}{\\sum_{i=1}^N w_{im}}%\n",
    "\\sum_{i=1}^N w_{im}\\mathbb{I}(\\tilde{y}_i\\neq F_m(\\boldsymbol{x}_i))$$\n",
    "**Adaboost** halla $F_m(\\cdot)$ y $\\beta_m$ en la iteración $m$. Los pesos de los datos para la primera iteración son $w_{i1}=1/N$. Una vez hallados $F_m(\\cdot)$ y $\\beta_m$ en la iteración $m$, los pesos de los datos para la iteración $m+1$ se calculan fácilmente:\n",
    "$$\\begin{align*}\n",
    "w_{i,m+1}&=\\exp(-\\tilde{y}_i f_m(\\boldsymbol{x}_i))\\\\%\n",
    "&=\\exp(-\\tilde{y}_i f_{m-1}(\\boldsymbol{x}_i)-\\tilde{y}_i\\beta_m F_m(\\boldsymbol{x}_i))\\\\%\n",
    "&=w_{im}\\exp(-\\tilde{y}_i\\beta_m F_m(\\boldsymbol{x}_i))\\\\%\n",
    "&=w_{im}\\exp(\\beta_m(2\\mathbb{I}(\\tilde{y}_i\\neq F_m(\\boldsymbol{x}_i))-1))\\\\%\n",
    "&=w_{im}\\exp(2\\beta_m\\mathbb{I}(\\tilde{y}_i\\neq F_m(\\boldsymbol{x}_i)))\\exp(-\\beta_m)\\\\%\n",
    "&=\\begin{cases}\n",
    "w_{im}\\exp(2\\beta_m) & \\text{si $\\tilde{y}_i\\neq F_m(\\boldsymbol{x}_i)$}\\\\%\n",
    "w_{im}               & \\text{en otro caso}%\n",
    "\\end{cases}\n",
    "\\end{align*}$$\n",
    "El modelo ajustado para clasificación binaria es $f(\\boldsymbol{x})=\\operatorname{sgn}(\\sum_m\\beta_m F_m(\\boldsymbol{x}))$; para regresión y clasificación multi-clase se usan variantes convenientemente adaptadas. En general, AdaBoost es muy sensible a outliers ya que los pesos de los datos mal clasificados crecen exponencialmente.\n",
    "\n",
    "Otro aspecto que limita la aplicabilidad de AdaBoost es que no facilita la estimación probabilidades. En teoría, el riesgo de un modelo $f(\\boldsymbol{x})$ con pérdida exponencial es:\n",
    "$$\\mathbb{E}[\\exp(-\\tilde{y}f(\\boldsymbol{x}))\\mid\\boldsymbol{x}]%\n",
    "=p(\\tilde{y}=1\\mid\\boldsymbol{x})\\exp(-f(\\boldsymbol{x}))%\n",
    "+p(\\tilde{y}=-1\\mid\\boldsymbol{x})\\exp(f(\\boldsymbol{x}))$$\n",
    "Derivando el riesgo con respecto a $f(\\boldsymbol{x})$ e igualando a cero, obtenemos que el modelo de mínimo riesgo teórico halla la mitad de la log-odds:\n",
    "$$f(\\boldsymbol{x})=\\frac{1}{2}\\log\\frac{p(\\tilde{y}=1\\mid\\boldsymbol{x})}{p(\\tilde{y}=-1\\mid\\boldsymbol{x})}$$\n",
    "Nótese que este resultado justifica la aplicación del operador signo al modelo ajustado para clasificación binaria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**  clasificación de correos en spam y no-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import transforms, pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/data/Spam.txt?raw=True\")\n",
    "target = 'spam'\n",
    "columns = ['word_freq_make', 'word_freq_address', 'word_freq_all',\n",
    "           'word_freq_3d', 'word_freq_our', 'word_freq_over',\n",
    "           'word_freq_remove', 'word_freq_internet', 'word_freq_order',\n",
    "           'word_freq_mail', 'word_freq_receive', 'word_freq_will',\n",
    "           'word_freq_people', 'word_freq_report', 'word_freq_addresses',\n",
    "           'word_freq_free', 'word_freq_business', 'word_freq_email',\n",
    "           'word_freq_you', 'word_freq_credit', 'word_freq_your',\n",
    "           'word_freq_font', 'word_freq_000', 'word_freq_money',\n",
    "           'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
    "           'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
    "           'word_freq_telnet', 'word_freq_857', 'word_freq_data',\n",
    "           'word_freq_415', 'word_freq_85', 'word_freq_technology',\n",
    "           'word_freq_1999', 'word_freq_parts', 'word_freq_pm',\n",
    "           'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
    "           'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
    "           'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
    "           'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
    "           'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
    "           'capital_run_length_longest', 'capital_run_length_total']\n",
    "X, y = df[columns].values, df[target].values\n",
    "is_test = df.test.values\n",
    "X_train, X_test = X[is_test == 0], X[is_test == 1]\n",
    "y_train, y_test = y[is_test == 0], y[is_test == 1]\n",
    "ntrees_list = [10, 50, 100, 200, 300, 400, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoosting 10 trees, test err 10.9%\n",
      "AdaBoosting 50 trees, test err 7.2%\n",
      "AdaBoosting 100 trees, test err 5.9%\n",
      "AdaBoosting 200 trees, test err 5.7%\n",
      "AdaBoosting 300 trees, test err 5.7%\n",
      "AdaBoosting 400 trees, test err 5.5%\n",
      "AdaBoosting 500 trees, test err 5.5%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for ntrees in ntrees_list:\n",
    "    clf = AdaBoostClassifier(n_estimators=ntrees, random_state=10, learning_rate=0.2).fit(X_train, y_train)\n",
    "    y_test_hat = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_test_hat)\n",
    "    print(f'AdaBoosting {ntrees} trees, test err {1 - acc:.1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
