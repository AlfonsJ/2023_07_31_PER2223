{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado aditivo por etapas hacia adelante\n",
    "\n",
    "**Forward stagewise additive modeling (FSAM)** optimiza la pérdida empı́rica con pérdida general y $f$ modelo aditivo. En la iteración $m$ calcula:\n",
    "$$(\\beta_m,\\boldsymbol{\\theta}_m)%\n",
    "=\\operatorname*{argmin}_{\\beta,\\boldsymbol{\\theta}}\\;L_m(\\beta,\\boldsymbol{\\theta})%\n",
    "\\qquad\\text{con}\\qquad%\n",
    "L_m(\\beta,\\boldsymbol{\\theta})%\n",
    "=\\sum_{i=1}^N\\ell(y_i, f_{m-1}(\\boldsymbol{x}_i)+\\beta F(\\boldsymbol{x}_i;\\boldsymbol{\\theta}))$$\n",
    "y reajusta el modelo,\n",
    "$$f_m(\\boldsymbol{x})=f_{m-1}(\\boldsymbol{x})+\\beta_m F_m(\\boldsymbol{x})%\n",
    "\\qquad\\text{con}\\qquad%\n",
    "F_m(\\boldsymbol{x})=F(\\boldsymbol{x};\\boldsymbol{\\theta}_m)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pérdida cuadrática y boosting mínimos cuadrados\n",
    "\n",
    "Si $\\ell(y,\\hat{y})=(y-\\hat{y})^2$, el objetivo en la iteración $m$ es:\n",
    "$$L_m(\\beta,\\boldsymbol{\\theta})%\n",
    "=\\sum_{i=1}^N (r_{im}-\\beta F(\\boldsymbol{x}_i;\\boldsymbol{\\theta}))^2\n",
    "\\qquad\\text{con}\\qquad r_{im}=y_i-f_{m-1}(\\boldsymbol{x}_i)$$\n",
    "**Boosting mínimos cuadrados** minimiza el objetivo fijando $\\beta=1$ y ajustando $F$ a los residuos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pérdida exponencial y AdaBoost\n",
    "\n",
    "Si $\\ell(\\tilde{y},\\hat{y})=\\exp(-\\tilde{y}\\hat{y})$, $\\tilde{y}\\in\\{-1,+1\\}$, el objetivo en la iteración $m$ es:\n",
    "$$\\begin{align*}\n",
    "L_m(\\beta,\\boldsymbol{\\theta})%\n",
    "&=\\sum_{i=1}^N \\exp(-\\tilde{y}_i(f_{m-1}(\\boldsymbol{x}_i)+\\beta F(\\boldsymbol{x}_i;\\boldsymbol{\\theta})))\\\\%\n",
    "&=\\sum_{i=1}^N w_{im}\\exp(-\\beta\\tilde{y}_i F(\\boldsymbol{x}_i;\\boldsymbol{\\theta}))%\n",
    "\\qquad\\text{con}\\qquad w_{im}=\\exp(-\\tilde{y}_i f_{m-1}(\\boldsymbol{x}_i))\\\\%\n",
    "&=e^{-\\beta}\\sum_{\\tilde{y}_i=F(\\boldsymbol{x}_i;\\boldsymbol{\\theta})} w_{im}%\n",
    "+e^{\\beta}\\sum_{\\tilde{y}_i\\neq F(\\boldsymbol{x}_i;\\boldsymbol{\\theta})} w_{im}\\\\%\n",
    "&=(e^{\\beta}-e^{-\\beta})\\sum_{i=1}^N w_{im}\\mathbb{I}(\\tilde{y}_i\\neq F(\\boldsymbol{x}_i;\\boldsymbol{\\theta}))%\n",
    "+e^{-\\beta}\\sum_{i=1}^N w_{im}\n",
    "\\end{align*}$$\n",
    "La minimización de $L_m$ puede hacerse en dos pasos. Primero hallamos $\\boldsymbol{\\theta}_m$ a partir de los datos ponderados:\n",
    "$$\\boldsymbol{\\theta}_m=\\operatorname*{argmin}_{\\boldsymbol{\\theta}}%\n",
    "\\sum_{i=1}^N w_{im}\\mathbb{I}(\\tilde{y}_i\\neq F(\\boldsymbol{x}_i;\\boldsymbol{\\theta}))$$\n",
    "y luego $\\beta_m$ mediante minimización en $\\beta$ de $L_m(\\beta,\\boldsymbol{\\theta}_m)$:\n",
    "$$\\beta_m=\\operatorname*{argmin}_{\\beta}\\;L_m(\\beta,\\boldsymbol{\\theta}_m)%\n",
    "=\\frac{1}{2}\\log\\frac{1-\\operatorname{err}_m}{\\operatorname{err}_m}%\n",
    "\\qquad\\text{con}\\qquad\\operatorname{err}_m=\\frac{1}{\\sum_{i=1}^N w_{im}}%\n",
    "\\sum_{i=1}^N w_{im}\\mathbb{I}(\\tilde{y}_i\\neq F_m(\\boldsymbol{x}_i))$$\n",
    "**Adaboost** halla $F_m(\\cdot)$ y $\\beta_m$ en la iteración $m$. Los pesos de los datos para la primera iteración son $w_{i1}=1/N$. Una vez hallados $F_m(\\cdot)$ y $\\beta_m$ en la iteración $m$, los pesos de los datos para la iteración $m+1$ se calculan fácilmente:\n",
    "$$\\begin{align*}\n",
    "w_{i,m+1}&=\\exp(-\\tilde{y}_i f_m(\\boldsymbol{x}_i))\\\\%\n",
    "&=\\exp(-\\tilde{y}_i f_{m-1}(\\boldsymbol{x}_i)-\\tilde{y}_i\\beta_m F_m(\\boldsymbol{x}_i))\\\\%\n",
    "&=w_{im}\\exp(-\\tilde{y}_i\\beta_m F_m(\\boldsymbol{x}_i))\\\\%\n",
    "&=w_{im}\\exp(\\beta_m(2\\mathbb{I}(\\tilde{y}_i\\neq F_m(\\boldsymbol{x}_i))-1))\\\\%\n",
    "&=w_{im}\\exp(2\\beta_m\\mathbb{I}(\\tilde{y}_i\\neq F_m(\\boldsymbol{x}_i)))\\exp(-\\beta_m)\\\\%\n",
    "&=\\begin{cases}\n",
    "w_{im}\\exp(2\\beta_m) & \\text{si $\\tilde{y}_i\\neq F_m(\\boldsymbol{x}_i)$}\\\\%\n",
    "w_{im}               & \\text{en otro caso}%\n",
    "\\end{cases}\n",
    "\\end{align*}$$\n",
    "El modelo ajustado para clasificación binaria es $f(\\boldsymbol{x})=\\operatorname{sgn}(\\sum_m\\beta_m F_m(\\boldsymbol{x}))$; para regresión y clasificación multi-clase se usan variantes convenientemente adaptadas. En general, AdaBoost es muy sensible a outliers ya que los pesos de los datos mal clasificados crecen exponencialmente.\n",
    "\n",
    "Otro aspecto que limita la aplicabilidad de AdaBoost es que no facilita la estimación probabilidades. En teoría, el riesgo de un modelo $f(\\boldsymbol{x})$ con pérdida exponencial es:\n",
    "$$\\mathbb{E}[\\exp(-\\tilde{y}f(\\boldsymbol{x}))\\mid\\boldsymbol{x}]%\n",
    "=p(\\tilde{y}=1\\mid\\boldsymbol{x})\\exp(-f(\\boldsymbol{x}))%\n",
    "+p(\\tilde{y}=-1\\mid\\boldsymbol{x})\\exp(f(\\boldsymbol{x}))$$\n",
    "Derivando el riesgo con respecto a $f(\\boldsymbol{x})$ e igualando a cero, obtenemos que el modelo de mínimo riesgo teórico halla la mitad de la log-odds:\n",
    "$$f(\\boldsymbol{x})=\\frac{1}{2}\\log\\frac{p(\\tilde{y}=1\\mid\\boldsymbol{x})}{p(\\tilde{y}=-1\\mid\\boldsymbol{x})}$$\n",
    "Nótese que este resultado justifica la aplicación del operador signo al modelo ajustado para clasificación binaria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-pérdida y LogitBoost\n",
    "\n",
    "Con el fin de facilitar la estimación de probabilidades, el modelo aditivo puede usarse para predecir la mitad de la log-odds:\n",
    "$$p(\\tilde{y}\\mid\\boldsymbol{x};\\boldsymbol{\\theta})%\n",
    "=\\sigma(\\tilde{y}a)\\qquad\\text{con}\\qquad a=2f(\\boldsymbol{x};\\boldsymbol{\\theta})$$\n",
    "Si tomamos la log-pérdida, $\\ell(\\tilde{y},\\boldsymbol{\\theta};\\boldsymbol{x})=-\\log p(\\tilde{y}\\mid\\boldsymbol{x};\\boldsymbol{\\theta})$, el objetivo en la iteración $m$ es, con $\\beta=1$:\n",
    "$$\\begin{align*}\n",
    "L_m(\\boldsymbol{\\theta})%\n",
    "&=-\\sum_{i=1}^N \\log(\\sigma(\\tilde{y}_i2[f_{m-1}(\\boldsymbol{x}_i)+ F(\\boldsymbol{x}_i;\\boldsymbol{\\theta})]))\\\\%\n",
    "&=\\sum_{i=1}^N \\log(1+\\exp(-2\\tilde{y}_i[f_{m-1}(\\boldsymbol{x}_i)+ F(\\boldsymbol{x}_i;\\boldsymbol{\\theta})]))\n",
    "\\end{align*}$$\n",
    "**LogitBoost** es un algoritmo de Newton para minimizar este objetivo directamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
