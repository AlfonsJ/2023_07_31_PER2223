{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.01 Árboles de regresión y clasificación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Los árboles de clasificación y regresión (CART) particionan el espacio de entrada recursivamente y definen un modelo local en cada región resultante; globalmente, el modelo és un árbol con una hoja por región. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. El árbol constituye un conjunto de reglas de decisión anidadas.\n",
    "2. Los nodos internos definen splits paralelos a los ejes.\n",
    "3. En cada hoja se especifica la salida predicha para toda entrada dentro de su región asociada.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Un árbol de regresión de $J$ hojas y parámetros $\\boldsymbol{\\theta}=\\{(R_j,w_j)\\}$ es $f(\\boldsymbol{x};\\boldsymbol{\\theta})=\\sum_{j=1}^Jw_j\\mathbb{I}(\\boldsymbol{x}\\in R_j)$, donde $R_j$ y $w_j$ denotan la región y salida asociadas a la hoja $j$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. El árbol constituye un conjunto de reglas de decisión anidadas.\n",
    "2. Los nodos internos definen splits paralelos a los ejes.\n",
    "3. En cada hoja se especifica la salida predicha para toda entrada dentro de su región asociada.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** El ajuste de un árbol $f(\\boldsymbol{x};\\boldsymbol{\\theta})=\\sum_{j=1}^Jw_j\\mathbb{I}(\\boldsymbol{x}\\in R_j)$, $\\boldsymbol{\\theta}=\\{(R_j,w_j)\\}$, requiere minimizar la pérdida empírica\n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta})=\\sum_{n=1}^N\\ell(y_n,f(\\boldsymbol{x}_n;\\boldsymbol{\\theta}))%\n",
    "=\\sum_{j=1}^J\\sum_{\\boldsymbol{x}_n\\in R_j}\\ell(y_n,w_j)$$\n",
    "Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. $\\mathcal{L}(\\boldsymbol{\\theta})$ no es diferenciable pues depende de la estructura del árbol.\n",
    "2. Hallar una estructura óptima del árbol es NP-Duro.\n",
    "3. La estructura del árbol se suele aprender con un método voraz como CART, C4.5 o ID3, que haga crecer el árbol añadiendo un nodo en cada iteración.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Los posibles splits de datos en un nodo $i$, $\\mathcal{D}_i=\\{(\\boldsymbol{x}_n,y_n)\\in N_i\\}$, dependen de la característica $j$ que se escoja. En general, se define un conjunto de valores de $j$, $\\mathcal{T}_j$, y por cada $t\\in\\mathcal{T}_j$ se considera una dicotomía de $\\mathcal{D}_i$ en un hijo izquierdo $\\mathcal{D}_i^L(j,t)$ y otro derecho $\\mathcal{D}_i^R(j,t)$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Si $j$ es real, $\\mathcal{T}_j$ se suele definir ordenando los valores únicos de $\\{x_{nj}\\}$; $\\mathcal{D}_i^L(j,t)$ incluye los datos de $i$ cuya característica $j$ no es mayor que $t$, mientras que el resto de datos va a $\\mathcal{D}_i^R(j,t)$.\n",
    "2. Si $j$ es categórica, $\\mathcal{T}_j$ se suele definir con todos sus valores posibles; $\\mathcal{D}_i^L(j,t)$ incluye los datos de $i$ cuya característica $j$ es igual a $t$, mientras que el resto de datos va a $\\mathcal{D}_i^R(j,t)$.\n",
    "3. Tanto si $j$ es real como si no, conviene excluir toda dicotomía que dé lugar a un hijo vacío.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** El mejor par característica-valor para hacer el split de un nodo $i$, $(j_i,t_i)$, se elige usando una función coste de nodo, $c(\\mathcal{D}_i)$:\n",
    "$$(j_i,t_i)=\\operatorname*{argmin}_{j\\in\\{1,\\dotsc,D\\}}\\min_{t\\in\\mathcal{T}_j}%\n",
    "\\frac{\\lvert\\mathcal{D}_i^L(j,t)\\rvert}{\\lvert\\mathcal{D}_i\\rvert}\\,c(\\mathcal{D}_i^L(j,t))%\n",
    "+\\frac{\\lvert\\mathcal{D}_i^R(j,t)\\rvert}{\\lvert\\mathcal{D}_i\\rvert}\\,c(\\mathcal{D}_i^R(j,t))$$\n",
    "Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. En regresión se usa $c(\\mathcal{D}_i)=\\frac{1}{\\lvert\\mathcal{D}_i\\rvert}\\sum\\nolimits_{n\\in\\mathcal{D}_i}(y_n-\\bar{y})^2$, donde $\\bar{y}$ denota la respuesta media en el nodo; esto es, el error cuadrático medio.\n",
    "2. En clasificación es popular el uso del índice Gini, que estima el error de clasificación esperado.\n",
    "3. En clasificación es popular el uso de la impureza, medida como entropía de la distribución empírica de les clases.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Si el árbol es demasiado grande, su ajuste puede conducir a sobre-entrenamiento, por lo que conviene aplicar alguna técnica de regularización que lo evite. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. El sobre-entrenamiento extremo produce error de entrenamiento nulo (sin ruido de etiquetas) tras particionar el espacio de entrada en regiones muy pequeñas de salida constante.\n",
    "2. Una técnica de regularización bien conocida consiste en parar el crecimiento con un heurístico, como tener pocos ejemplos en un nodo o alcanzar una profundidad máxima.\n",
    "3. Otra técnica de regularización consiste en dejar crecer el árbol al máximo y podarlo de hijos a padres mediante fusión de hijos.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** A diferencia de otros modelos (como las redes), los árboles manejan bien características de entrada perdidas mediante heurísticos sencillos. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Si en test nos falta una variable, usamos splits surrogados, esto es, una variable sustituta que induzca particiones similares.\n",
    "2. Si dos o más características se hallan muy correladas en un mismo nodo, es de esperar que induzcan particiones similares.\n",
    "3. Con variables categóricas, se suele codificar \"perdido\" como nuevo valor y tratar los datos como completamente observados.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Los árboles presentan una serie de ventajas e incovenientes que cabe tener presentes para su uso. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Son insensibles a transformaciones monótonas de las entradas ya que los puntos de split se basan en la ordenación de los datos, por lo que no es necesario estandarizarlos.\n",
    "2. Son de ajuste rápido y fácil escalado a muchos datos.\n",
    "3. Son inestables, esto es, pequeños cambios en los datos de entrada pueden tener grandes cambios en la estructura del árbol.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.02 Aprendizaje de ensambles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Los árboles constituyen un estimador de alta varianza ya que pequeñas perturbaciones de los datos de entrenamiento resultan en predicciones muy distintas. El aprendizaje de ensambles reduce la varianza de los árboles promediando $\\lvert\\mathcal{M}\\rvert$ modelos base $\\{f_m\\}$, $f(y\\mid\\boldsymbol{x})=\\frac{1}{\\lvert\\mathcal{M}\\rvert}\\sum\\nolimits_{m\\in\\mathcal{M}}f_m(y\\mid\\boldsymbol{x})$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. En regresión, el promediado suele presentar un sesgo similar al de los modelos base, pero mejor precisión por la menor varianza.\n",
    "2. En clasificación se usa el voto mayoritario o método comité, que incrementa la precisión de los modelos base, especialmemte cuando sus errores de predicción no se hallan correlados.\n",
    "3. La precisión de un comité de $M$ predictores independientes de probabilidad de acierto $\\theta$ puede calcularse fácilmente a partir de la función de distribución binomial como $p=1-B(M/2,M,\\theta)$.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.03 Bagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Bagging (bootstrap aggregating) ensambla $M$ modelos ajustados con diferentes versiones de los datos, obtenidas por boostraping (muestreo con reemplazamiento). Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Una desventaja es que cada modelo solo ve un $63\\%$ de datos aprox, si bien los datos no vistos pueden usarse en test.\n",
    "2. Una ventaja es que ofrece mayor robustez y generalización que ensamble pues no depende demasiado de ningún dato de entrenamiento individual.\n",
    "3. Aunque funciona bien con árboles, no suele mejorar la precisión de otros modelos como vecinos y redes.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.04 Random forests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Los bosques aleatorios decorrelan los aprendices mediante aleatorización, no solo de los datos de entrenamiento, sino también de las variables de entrada. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. La aleatorización de variables se realiza a nivel de nodo: la característica de split $j_i$ de un nodo $i$ se optimiza sobre un subconjunto aleatorio $S_i\\subseteq\\{1,\\dotsc,D\\}$.\n",
    "2. En general, los bosques son más precisos que bagging ya que muchas características son irrelevantes.\n",
    "3. En comparación con boosting, cuyo entrenamiento es esencialmente secuencial, los (aprendices de los) bosques pueden entrenarse en paralelo.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.05 Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Llamamos modelo aditivo al ensamble de funciones base aditivas generales, $f(\\boldsymbol{x};\\boldsymbol{\\theta})=\\sum\\nolimits_{m=1}^M\\beta_mF_m(\\boldsymbol{x};\\boldsymbol{\\theta}_m)$, ajustable mediante minimización de la pérdida empírica, $\\mathcal{L}(f)=\\sum\\nolimits_{i=1}^N\\ell(y_i,f(\\boldsymbol{x}_i))$. Boosting ajusta secuencialmente modelos aditivos de clasificadores binarios, $F_m\\in\\{-1,+1\\}$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Boosting ajusta $F_1$ y pondera los datos asignando mayor peso a los errores; luego ajusta $F_2$ a los datos ponderados, repondera, y repite ajuste y reponderación hasta llegar a $M$ componentes.\n",
    "2. El strong learner, $f$, mejora la precisión de los weak learners, $\\{F_m\\}$, si la precisión de cada uno de ellos es mejor que el azar.\n",
    "3. Boosting va mejor que bagging y bosques ya que reduce el sesgo de $f$ ajustando árboles que dependen unos de otros, en lugar de limitarse a reducir la varianza con árboles independientes.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Forward stagewise additive modeling (FSAM) optimiza la pérdida empírica con pérdida general y $f$ modelo aditivo. En la iteración $m$, calcula $(\\beta_m,\\boldsymbol{\\theta}_m)$ y reajusta modelo, $f_m(\\boldsymbol{x})=f_{m-1}(\\boldsymbol{x})+\\beta_mF(\\boldsymbol{x};\\boldsymbol{\\theta}_m)$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Aunque boosting se propuso en los 90 en el marco del aprendizaje PAC (probablemente aproximadamente correcto), poco después se interpretó de forma más general, bajo una visión estadística, como técnica FSAM.\n",
    "2. No reajusta parámetros de modelos base añadidos antes.\n",
    "3. Los detalles de optimización dependen de la pérdida escogida y (en algunos casos) de la forma del aprendiz débil $F$.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** A continuación se describen algunos algoritmos de boosting en relación con FSAM. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Boosting mínimos cuadrados (L2Boost) es FSAM con pérdida cuadrática, $\\ell(y,\\hat{y})=(y-\\hat{y})^2$.\n",
    "2. AdaBoost es FSAM para clasificación binaria, $\\tilde{y}_i\\in\\{-1,+1\\}$, con pérdida exponencial, $\\ell(\\tilde{y},F(\\boldsymbol{x}))=\\exp(-\\tilde{y}F(\\boldsymbol{x}))$.\n",
    "3. LogitBoost es FSAM para clasificación binaria, $\\tilde{y}_i\\in\\{-1,+1\\}$, con logloss, $\\ell(\\tilde{y},F(\\boldsymbol{x}))=\\log(1+e^{-2\\tilde{y}F(\\boldsymbol{x})})$.\n",
    "4. Todas son correctas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
