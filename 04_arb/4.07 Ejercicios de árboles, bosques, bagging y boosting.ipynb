{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.01 Árboles de regresión y clasificación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Los árboles de clasificación y regresión (CART) particionan el espacio de entrada recursivamente y definen un modelo local en cada región resultante; globalmente, el modelo és un árbol con una hoja por región. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. El árbol constituye un conjunto de reglas de decisión anidadas.\n",
    "2. Los nodos internos definen splits paralelos a los ejes.\n",
    "3. En cada hoja se especifica la salida predicha para toda entrada dentro de su región asociada.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Un árbol de regresión de $J$ hojas y parámetros $\\boldsymbol{\\theta}=\\{(R_j,w_j)\\}$ es $f(\\boldsymbol{x};\\boldsymbol{\\theta})=\\sum_{j=1}^Jw_j\\mathbb{I}(\\boldsymbol{x}\\in R_j)$, donde $R_j$ y $w_j$ denotan la región y salida asociadas a la hoja $j$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. El árbol constituye un conjunto de reglas de decisión anidadas.\n",
    "2. Los nodos internos definen splits paralelos a los ejes.\n",
    "3. En cada hoja se especifica la salida predicha para toda entrada dentro de su región asociada.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** El ajuste de un árbol $f(\\boldsymbol{x};\\boldsymbol{\\theta})=\\sum_{j=1}^Jw_j\\mathbb{I}(\\boldsymbol{x}\\in R_j)$, $\\boldsymbol{\\theta}=\\{(R_j,w_j)\\}$, requiere minimizar la pérdida empírica\n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta})=\\sum_{n=1}^N\\ell(y_n,f(\\boldsymbol{x}_n;\\boldsymbol{\\theta}))%\n",
    "=\\sum_{j=1}^J\\sum_{\\boldsymbol{x}_n\\in R_j}\\ell(y_n,w_j)$$\n",
    "Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. $\\mathcal{L}(\\boldsymbol{\\theta})$ no es diferenciable pues depende de la estructura del árbol.\n",
    "2. Hallar una estructura óptima del árbol es NP-Duro.\n",
    "3. La estructura del árbol se suele aprender con un método voraz como CART, C4.5 o ID3, que haga crecer el árbol añadiendo un nodo en cada iteración.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Los posibles splits de datos en un nodo $i$, $\\mathcal{D}_i=\\{(\\boldsymbol{x}_n,y_n)\\in N_i\\}$, dependen de la característica $j$ que se escoja. En general, se define un conjunto de valores de $j$, $\\mathcal{T}_j$, y por cada $t\\in\\mathcal{T}_j$ se considera una dicotomía de $\\mathcal{D}_i$ en un hijo izquierdo $\\mathcal{D}_i^L(j,t)$ y otro derecho $\\mathcal{D}_i^R(j,t)$. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Si $j$ es real, $\\mathcal{T}_j$ se suele definir ordenando los valores únicos de $\\{x_{nj}\\}$; $\\mathcal{D}_i^L(j,t)$ incluye los datos de $i$ cuya característica $j$ no es mayor que $t$, mientras que el resto de datos va a $\\mathcal{D}_i^R(j,t)$.\n",
    "2. Si $j$ es categórica, $\\mathcal{T}_j$ se suele definir con todos sus valores posibles; $\\mathcal{D}_i^L(j,t)$ incluye los datos de $i$ cuya característica $j$ es igual a $t$, mientras que el resto de datos va a $\\mathcal{D}_i^R(j,t)$.\n",
    "3. Tanto si $j$ es real como si no, conviene excluir toda dicotomía que dé lugar a un hijo vacío.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** El mejor par característica-valor para hacer el split de un nodo $i$, $(j_i,t_i)$, se elige usando una función coste de nodo, $c(\\mathcal{D}_i)$:\n",
    "$$(j_i,t_i)=\\operatorname*{argmin}_{j\\in\\{1,\\dotsc,D\\}}\\min_{t\\in\\mathcal{T}_j}%\n",
    "\\frac{\\lvert\\mathcal{D}_i^L(j,t)\\rvert}{\\lvert\\mathcal{D}_i\\rvert}\\,c(\\mathcal{D}_i^L(j,t))%\n",
    "+\\frac{\\lvert\\mathcal{D}_i^R(j,t)\\rvert}{\\lvert\\mathcal{D}_i\\rvert}\\,c(\\mathcal{D}_i^R(j,t))$$\n",
    "Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. En regresión se usa $c(\\mathcal{D}_i)=\\frac{1}{\\lvert\\mathcal{D}_i\\rvert}\\sum\\nolimits_{n\\in\\mathcal{D}_i}(y_n-\\bar{y})^2$, donde $\\bar{y}$ denota la respuesta media en el nodo; esto es, el error cuadrático medio.\n",
    "2. En clasificación es popular el uso del índice Gini, que estima el error de clasificación esperado.\n",
    "3. En clasificación es popular el uso de la impureza, medida como entropía de la distribución empírica de les clases.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Si el árbol es demasiado grande, su ajuste puede conducir a sobre-entrenamiento, por lo que conviene aplicar alguna técnica de regularización que lo evite. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. El sobre-entrenamiento extremo produce error de entrenamiento nulo (sin ruido de etiquetas) tras particionar el espacio de entrada en regiones muy pequeñas de salida constante.\n",
    "2. Una técnica de regularización bien conocida consiste en parar el crecimiento con un heurístico, como tener pocos ejemplos en un nodo o alcanzar una profundidad máxima.\n",
    "3. Otra técnica de regularización consiste en dejar crecer el árbol al máximo y podarlo de hijos a padres mediante fusión de hijos.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** A diferencia de otros modelos (como las redes), los árboles manejan bien características de entrada perdidas mediante heurísticos sencillos. Indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Si en test nos falta una variable, usamos splits surrogados, esto es, una variable sustituta que induzca particiones similares.\n",
    "2. Si dos o más características se hallan muy correladas en un mismo nodo, es de esperar que induzcan particiones similares.\n",
    "3. Con variables categóricas, se suele codificar \"perdido\" como nuevo valor y tratar los datos como completamente observados.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
