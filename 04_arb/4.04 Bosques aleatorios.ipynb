{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests\n",
    "\n",
    "**Random forests** puede verse como una variante de bagging de árboles que trata de mejorar la decorrelación de modelos base mediante aleatorización, no solo de los datos de entrenamiento, sino también de las variables de entrada. Así, la característica de split $j_i$ se optimiza sobre un conjunto aleatorio $S_i\\subseteq\\{1,\\dotsc,D\\}$,\n",
    "$$(j_i,t_i)=\\operatorname*{arg}\n",
    "\\min_{j\\in S_i}\\min_{t\\in\\mathcal{T}_j}\\;%\n",
    "\\frac{\\lvert\\mathcal{D}_i^L(j,t)\\rvert}{\\lvert\\mathcal{D}_i\\rvert}\\,c(\\mathcal{D}_i^L(j,t))+%\n",
    "\\frac{\\lvert\\mathcal{D}_i^R(j,t)\\rvert}{\\lvert\\mathcal{D}_i\\rvert}\\,c(\\mathcal{D}_i^R(j,t))$$\n",
    "En general, los bosques son más precisos que bagging ya que muchas características son irrelevantes. Por otro lado, los aprendices pueden entrenarse en paralelo, cosa que no puede hacerse con boosting.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** clasificación de correos en spam y no-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   test  word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0     1            0.00               0.64           0.64           0.0   \n",
       "1     0            0.21               0.28           0.50           0.0   \n",
       "2     1            0.06               0.00           0.71           0.0   \n",
       "3     0            0.00               0.00           0.00           0.0   \n",
       "4     0            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  ...  char_freq_;  char_freq_(  char_freq_[  char_freq_!  \\\n",
       "0             0.00  ...         0.00        0.000          0.0        0.778   \n",
       "1             0.00  ...         0.00        0.132          0.0        0.372   \n",
       "2             0.64  ...         0.01        0.143          0.0        0.276   \n",
       "3             0.31  ...         0.00        0.137          0.0        0.137   \n",
       "4             0.31  ...         0.00        0.135          0.0        0.135   \n",
       "\n",
       "   char_freq_$  char_freq_#  capital_run_length_average  \\\n",
       "0        0.000        0.000                       3.756   \n",
       "1        0.180        0.048                       5.114   \n",
       "2        0.184        0.010                       9.821   \n",
       "3        0.000        0.000                       3.537   \n",
       "4        0.000        0.000                       3.537   \n",
       "\n",
       "   capital_run_length_longest  capital_run_length_total  spam  \n",
       "0                          61                       278     1  \n",
       "1                         101                      1028     1  \n",
       "2                         485                      2259     1  \n",
       "3                          40                       191     1  \n",
       "4                          40                       191     1  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import transforms, pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/data/Spam.txt?raw=True\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'spam'\n",
    "columns = ['word_freq_make', 'word_freq_address', 'word_freq_all',\n",
    "           'word_freq_3d', 'word_freq_our', 'word_freq_over',\n",
    "           'word_freq_remove', 'word_freq_internet', 'word_freq_order',\n",
    "           'word_freq_mail', 'word_freq_receive', 'word_freq_will',\n",
    "           'word_freq_people', 'word_freq_report', 'word_freq_addresses',\n",
    "           'word_freq_free', 'word_freq_business', 'word_freq_email',\n",
    "           'word_freq_you', 'word_freq_credit', 'word_freq_your',\n",
    "           'word_freq_font', 'word_freq_000', 'word_freq_money',\n",
    "           'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
    "           'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
    "           'word_freq_telnet', 'word_freq_857', 'word_freq_data',\n",
    "           'word_freq_415', 'word_freq_85', 'word_freq_technology',\n",
    "           'word_freq_1999', 'word_freq_parts', 'word_freq_pm',\n",
    "           'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
    "           'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
    "           'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
    "           'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
    "           'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
    "           'capital_run_length_longest', 'capital_run_length_total']\n",
    "X, y = df[columns].values, df[target].values\n",
    "is_test = df.test.values\n",
    "X_train, X_test = X[is_test == 0], X[is_test == 1]\n",
    "y_train, y_test = y[is_test == 0], y[is_test == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged 10 trees, test err 5.9%\n",
      "Bagged 50 trees, test err 5.5%\n",
      "Bagged 100 trees, test err 5.4%\n",
      "Bagged 200 trees, test err 5.5%\n",
      "Bagged 300 trees, test err 5.5%\n",
      "Bagged 400 trees, test err 5.4%\n",
      "Bagged 500 trees, test err 5.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "ntrees_list = [10, 50, 100, 200, 300, 400, 500]\n",
    "for ntrees in ntrees_list:\n",
    "    bag_clf = BaggingClassifier(n_estimators=ntrees, random_state=10, bootstrap=True).fit(X_train, y_train)\n",
    "    y_test_hat = bag_clf.predict(X_test)\n",
    "    bag_acc = accuracy_score(y_test, y_test_hat)\n",
    "    print(f'Bagged {ntrees} trees, test err {1 - bag_acc:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 10 trees, test err 6.3%\n",
      "RF 50 trees, test err 5.0%\n",
      "RF 100 trees, test err 4.9%\n",
      "RF 200 trees, test err 4.8%\n",
      "RF 300 trees, test err 4.9%\n",
      "RF 400 trees, test err 4.8%\n",
      "RF 500 trees, test err 4.8%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for ntrees in ntrees_list:\n",
    "    rf_clf = RandomForestClassifier(n_estimators=ntrees, random_state=10).fit(X_train, y_train)\n",
    "    y_test_hat = rf_clf.predict(X_test)\n",
    "    rf_acc = accuracy_score(y_test, y_test_hat)\n",
    "    print(f'RF {ntrees} trees, test err {1 - rf_acc:.1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
