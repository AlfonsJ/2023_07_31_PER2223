{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje de métricas distancia\n",
    "\n",
    "La **\"distancia semántica\"** entre un par de puntos, $d(\\boldsymbol{x},\\boldsymbol{x}')\\in\\mathbb{R}^{\\geq 0}$ para $\\boldsymbol{x},\\boldsymbol{x}'\\in\\mathcal{X}$, o equivalentemente su similtud $s(\\boldsymbol{x},\\boldsymbol{x}')\\in\\mathbb{R}^{\\geq 0}$, es un concepto muy importante en clasificación KNN, aprendizaje auto-supervisado, clustering, recuperación de información, seguimiento visual, etc.\n",
    "\n",
    "Si $\\mathcal{X}=\\mathbb{R}^D$ y la dimensión $D$ no es muy alta, se suele emplear la distancia de Mahalanobis:\n",
    "$$d(\\boldsymbol{x},\\boldsymbol{x}')=\\sqrt{(\\boldsymbol{x}-\\boldsymbol{x}')^tM(\\boldsymbol{x}-\\boldsymbol{x}')}$$\n",
    "donde $M$ se aprende directamente o, indirectamente, a través de una proyección lineal.\n",
    "\n",
    "En el caso de entradas de alta dimensión, se aprende un embedding $\\boldsymbol{e}=f(\\boldsymbol{x})$ con una red neuronal profunda y luego se calculan distancias en el espacio de embedding. Se pretende que dichas distancias sean altamente significativas y menos sujetas a la maldición de la dimensionalidad. Este caso, conocido como **deep metric learning,** ha recibido gran atención en los últimos años."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep metric learning (DML)\n",
    "\n",
    "Sea $\\boldsymbol{e}=f(\\boldsymbol{x};\\boldsymbol{\\theta})\\in\\mathbb{R}^L$ un embedding de la entrada en un espacio \"semántico\" de baja dimensión y sea $\\hat{\\boldsymbol{e}}=\\boldsymbol{e}/\\lVert\\boldsymbol{e}\\rVert_2$ su versión $\\ell_2$-normalizada. La $\\ell_2$-normalización garantiza que todos los puntos se hallen en la hiperesfera unitaria. Asumimos que la comparación entre dos puntos se realizará mediante la **distancia Euclídea normalizada** (con mayor similitud cuanto menor sea),\n",
    "$$d(\\boldsymbol{x}_i,\\boldsymbol{x}_j;\\boldsymbol{\\theta})%\n",
    "=\\lVert\\hat{\\boldsymbol{e}}_i-\\hat{\\boldsymbol{e}}_j\\rVert_2^2%\n",
    "=(\\hat{\\boldsymbol{e}}_i-\\hat{\\boldsymbol{e}}_j)^t(\\hat{\\boldsymbol{e}}_i-\\hat{\\boldsymbol{e}}_j)%\n",
    "=2-2\\hat{\\boldsymbol{e}}_i^t\\hat{\\boldsymbol{e}}_j$$\n",
    "o, equivalentemente, mediante la **similitud coseno** (con mayor similitud cuanto menor sea):\n",
    "$$d(\\boldsymbol{x}_i,\\boldsymbol{x}_j;\\boldsymbol{\\theta})%\n",
    "=\\hat{\\boldsymbol{e}}_i^t\\hat{\\boldsymbol{e}}_j$$\n",
    "\n",
    "DML debe aprender una función de embedding tal que los ejemplos similares estén más cerca que los ejemplos no similares. Más precisamente, supongamos que $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i):i=1:N\\}$ es un conjunto de datos etiquetados y que $S=\\{(i,j):y_i=y_j\\}$ es un conjunto de pares similares derivado. La propiedad básica que se desea cumplir es que, si $(i,j)\\in S$ pero $(i,k)\\not\\in S$, entonces $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_j$ deben estar cerca en el espacio de embedding, pero $\\boldsymbol{x}_i$ y $\\boldsymbol{x}_k$ deben estar lejos. Se han propuesto diversos criterios para cumplir esta propiedad; así como criterios para cumplirla con datos no etiquetados siempre que exista una manera de definir pares similares. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
