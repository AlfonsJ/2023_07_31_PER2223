{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.00 Modelos paramétricos y no paramétricos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Una dicotomı́a clásica entre modelos de aprendizaje automático distingue entre modelos paramétricos y no paramétricos. En relación con esta dicotomı́a, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Los paramétricos estiman un vector de parámetros de dimensión fija a partir de datos (de aprendizaje) y luego, en inferencia, prescinden de los datos.\n",
    "2. Los no parámetricos mantienen los datos (tras el aprendizaje, en inferencia), por lo que puede decirse que el número efectivo de parámetros crece con el número de datos.\n",
    "3. Los no parámetricos suelen definirse en términos de una medida de (di)similitud o función distancia para comparar datos.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.01 K-vecinos más próximos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema:** Sea un problema de clasificación de datos 2d en tres clases, $c\\in\\{1, 2, 3\\}$. Se tienen los siguientes datos de aprendizaje:\n",
    "$$\\begin{array}{cccc}\n",
    "n & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15\\\\\\hline%\n",
    "x_{n1} &1&1&1 &2&2&2&2 &3 &4&4 &6 &7&7&7 &8\\\\%\n",
    "x_{n2} &2&3&8 &1&3&7&8 &6 &5&6 &2 &1&2&3 &1\\\\\\hline%\n",
    "c_n    &1&1&2 &1&1&2&2 &2 &1&2 &3 &3&3&3 &3%\n",
    "\\end{array}$$\n",
    "Clasifica la muestra de test $\\boldsymbol{x}=(5, 5)^t$ por los $K$-vecinos más próximos en distancia Euclídea para $K=1,2,3,4,5$. En caso de empate de votos, desempata con NN entre las clases empatadas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** Hallamos las distancias Euclídeas al cuadrado entre $\\boldsymbol{x}$ y cada dato:\n",
    "$$\\begin{array}{cccc}\n",
    "n & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15\\\\\\hline%\n",
    "d(\\boldsymbol{x}_n,\\boldsymbol{x}) &25&20&25 &25&13&13&18 &5 &1&2 &10 &20&13&8&25%\n",
    "\\end{array}$$\n",
    "El conjunto $5$ vecinos más próximos de $\\boldsymbol{x}$ es $N_5(\\boldsymbol{x})=\\{9, 10, 8, 14, 11\\}$. Nótese que este conjunto es único; si hubiera algún dato no incluido en este conjunto a distancia al cuadrado $10$, tendríamos otro posible conjunto $5$ vecinos más próximos de $\\boldsymbol{x}$ y escogeríamos uno de ellos al azar.\n",
    "\n",
    "Los $5$ vecinos votan: $1$ a la clase $1$, $2$ a la $2$, y $2$ a la $3$. Observamos que no existe una única clase más votada; tenemos dos clases empatadas a dos votos, la $2$ y la $3$, por lo que debemos desempatar con el NN entre las clases empatadas. Entre los vecinos de las clases $2$ y $3$, el más cercano es el $10$, que pertenece a la clase $2$. Por tanto, $5$-NN clasifica $\\boldsymbol{x}$ en la clase $2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:**  El elevado coste espacial y temporal de KNN limita en gran medida su aplicabilidad a grandes conjuntos de datos. En relación con el mismo, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. Se han propuesto diversas para reducir el coste espacial eliminando datos que no afectan a las fronteras de decisión.\n",
    "2. Con el fin de reducir el coste temporal, se han propuesto numerosas técnicas de búsqueda eficiente de $K$ vecinos, exacta y aproximada (para $d>10$).\n",
    "3. **k-d tree** y **locality sensitive hashing (LSH)** son dos técnicas populares de búsqueda rápida de vecinos basadas en particionamiento del espacio en regiones y hashing, respectivamente.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuestión:** Una de las principales bondades de KNN viene dada por la relativa facilidad con la que puede aplicarse en tareas de reconocimiento (de conjunto) abierto. En relación con este tipo de tareas, indica la respuesta incorrecta (o escoge la última opción si las tres primeras son correctas).\n",
    "1. **Novelty detection:** El sistema detecta que la muestra de test es de una clase no vista antes; por ejemplo una cara desconocida.\n",
    "2. **Incremental learning, online learning, life-long learning o continual learning:** El sistema detecta una nueva clase con éxito, pregunta por el id de la nueva clase y la añade a las existentes.\n",
    "3. **Out-of-distribution detection:** Se detecta que la muestra de test no es de clase conocida ni desconocida, sino que procede de una distribución enteramente distinta; p.e., una foto sin cara.\n",
    "4. Todas son correctas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 4."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
